
#include <sys/types.h>
#include <sys/malloc.h>
#include <sys/param.h>
#include <sys/kernel.h>
#include <sys/param.h>
#include <sys/module.h>
#include <sys/kernel.h>
#include <sys/systm.h>
#include <sys/proc.h>
#include <sys/sysent.h>
#include <sys/kthread.h>
#include <sys/unistd.h>
#include <sys/smp.h>
#include <sys/sched.h>
#include <sys/lock.h>
#include <sys/mutex.h>
#include <vm/vm.h>
#include <vm/pmap.h>
#include <vm/vm_map.h>

#include <machine/cpu.h>
#include <machine/specialreg.h>
#include <machine/md_var.h>
#include <dev/hyperv/vmbus/x86/hyperv_machdep.h>

#include "vmxon64.h"
#include "vmx64.h"

#define CGCOS_PERS 0x41
#define CGCOS_SYS_MAXSYSCALL 8

static MALLOC_DEFINE(M_VMXON, "vmxon data", "vmxon Data");
static MALLOC_DEFINE(M_VMCS_GUEST, "vmcs data", "vmcs Data");
static MALLOC_DEFINE(M_VMCS_MSR, "vmcs data", "vmcs Data");
static MALLOC_DEFINE(M_HOSTSTACK, "host stack data", "host stack Data");
static MALLOC_DEFINE(M_VMXARRAY, "vmx array", "vmx Array");

#define STACK_PAGES 2

static uint8_t *msr_bitmap_region;

static VmxStruct **threadVmx = NULL;

static void cgc_vmxoff(uint32_t procNum);
static void vmexit_rdmsr(VmxStruct *vmx);
static void vmexit_wrmsr(VmxStruct *vmx);

static void set_msr_bitmap(uint32_t msr, int r, int w);
static void msrloadstore_init(VmxStruct *);
static void msrloadstore_init_entry(VmxStruct *, msrloadstore *);
#if 0
static int msrloadstore_add(VmxStruct *, msrloadstore *, uint32_t);
static int msrloadstore_addval(VmxStruct *, msrloadstore *, uint32_t, uint64_t);
#endif

static void cgcvmx_off(void);
int real_cgcvmx_on(uint64_t guestRip, uint64_t guestRsp, uint32_t procNum);

static uint32_t makeValidAccessRights(uint32_t sel) {
   uint32_t access;
   access = (GetSegAttr(sel) >> 8) & 0xf0ff;
   if ((access & ATTR_PRESENT) == 0) {
      access = ATTR_UNUSABLE;
   }
   else if ((access & ATTR_TYPE_ACCESSED) == 0) {
      printf("<cgcvmx> selector 0x%x appears to be in use, but the ACCESSED bit is not set\n", sel);
//      access |= ATTR_TYPE_ACCESSED;   //required by guest state checks (see Vol 3, 26.3.1.2)
   }
   return access;
}

/* fill in a descriptor struct given a selector */
static uint32_t segFromSelector(Segment *seg, uint16_t sel, uint8_t *gdt) {
   Descriptor *desc;
   if (sel & 0x4) {
      //LDT selector, not GDT selector
      return -1;
   }

   desc = (Descriptor*) (gdt + (sel & ~7));

   seg->sel = sel;
   seg->base = desc->base15 | (desc->base23 << 16) | (desc->base31 << 24);
   seg->limit = desc->limit15 | ((desc->attr & 0xf00) << 8);
   seg->attr = desc->attr & 0xf0ff;  //remove limit bits

   //we ARE running in IA-32e (64-bit) mode "system" descriptors
   //are 16 (vice 8) byte descriptors

   if (seg->attr & ATTR_GRANULARITY) {
      // 4096-bit granularity so scale the limit
      seg->limit = (seg->limit << 12) + 0xfff;
   }
   if ((seg->attr & ATTR_SYSTEM) == 0) {
      //this is a system descriptor
      //system descriptors in IA_32E mode are 64 bits
   }
   
   return 0;
}

static void cgc_vmwrite(uint64_t field, uint64_t value) {
   int x;

   if ((x = vm_write(field, value)) != 0) {
	 printf("<cgcvmx> vm_write(%lx, %lx) failed", field, value);
	 if (x & 0x40)
	    printf("(%lx)\n", vm_read(VMX_INSTR_ERROR));
	 else
	    printf("[invalid]\n");
   }
}

static void modifyReg(uint64_t *reg, uint64_t val, uint32_t size) {
   switch (size) {
      case 0:
         val &= 0xff;
         *reg = (*reg & ~0xffLL) | val;
         break;
      case 1:
         val &= 0xffff;
         *reg = (*reg & ~0xffffLL) | val;
         break;
      case 2:
         val &= 0xffffffff;
         *reg = (*reg & ~0xffffffffLL) | val;
         break;
      default:
         *reg = val;
         break;
   }
}

//Invalid Opcode Exception #UD is interrupt 6, no error code
#define INTR_DELIVER 0x80000000
#define HAS_ERROR_CODE 0x800
//vector 0..255
//types:
//    3 - Hardware exception
//    4 - software interrupt eg int 3
//    5 - privileged software exception
//    6 - software exception
/*
The interruption type (bits 10:8) determines details of how the injection is performed. In general, a VMM
should use the type hardware exception for all exceptions other than breakpoint exceptions (#BP;
generated by INT3) and overflow exceptions (#OF; generated by INTO); it should use the type software
exception for #BP and #OF. The type other event is used for injection of events that are not delivered
through the IDT.
*/
// injectInterrupt(X86_INVALID_OPCODE, INJECT_INTR_TYPE_HARDWARE_EXC, 1, 0, 0); //one byte undefined opcode

static void injectInterrupt(uint32_t vector, uint32_t type, uint32_t instLen, uint32_t useErrorCode, uint32_t errorCode) {
   //use for event injection
   //deliver bit gets cleared on vm exit
   uint32_t info = INTR_DELIVER | ((type & 7) << 8) | (vector & 0xff);
   if (useErrorCode) {
      info |= HAS_ERROR_CODE;
      cgc_vmwrite(VMX_ENTRY_EXCEPTION_EC, errorCode);
   }
   cgc_vmwrite(VMX_ENTRY_INT_INFO_FIELD, info);

   if (type >= 4) {
      //for software interrupts or exceptions
      cgc_vmwrite(VMX_ENTRY_INSTR_LENGTH, instLen);
   }
}

static int cgcvmx_register_by_index(unsigned long i, VmxStruct *vmx, uint64_t *r);
static int cgcvmx_register_by_index_put(unsigned long i, VmxStruct *vmx, uint64_t r);
static int cgcvmx_selector_by_index(unsigned long i, VmxStruct *vmx, uint16_t *r);
static int cgcvmx_handle_gdt_idt(struct proc *, VmxStruct *vmx);
static int cgcvmx_handle_ldt_tr(struct proc *, VmxStruct *vmx);
static int cgcvmx_copyout(struct proc *p, VmxStruct *vmx, uint16_t seg, uintptr_t off, void *ptr, int len);
static void cgcvmx_cpuid(struct proc *p, VmxStruct *vmx, CpuIdRegs *regs);

static int cgcvmx_selector_by_index(unsigned long i, VmxStruct *vmx, uint16_t *r) {
   if (i >= 6) {
      injectInterrupt(X86_INVALID_OPCODE,
            INJECT_INTR_TYPE_HARDWARE_EXC,
            vm_read(VMX_EXIT_INSTR_LEN), 0, 0);
      return (-1);
   }
   *r = vm_read(VMX_GUEST_ES_SEL + (i * 2));
   return (0);
}

static int cgcvmx_register_by_index(unsigned long i, VmxStruct *vmx, uint64_t *r) {
   if (i >= 8) {
      injectInterrupt(X86_INVALID_OPCODE,
            INJECT_INTR_TYPE_HARDWARE_EXC,
            vm_read(VMX_EXIT_INSTR_LEN), 0, 0);
      return (-1);
   }
   *r = (&vmx->rax)[-i];
   return (0);
}

static int cgcvmx_register_by_index_put(unsigned long i, VmxStruct *vmx, uint64_t r) {
   if (i >= 8) {
      injectInterrupt(X86_INVALID_OPCODE,
            INJECT_INTR_TYPE_HARDWARE_EXC,
            vm_read(VMX_EXIT_INSTR_LEN), 0, 0);
      return (-1);
   }
   (&vmx->rax)[-i] = r;
   return (0);
}

#define PROC_IS_CGC(p) (((p) != NULL) && \
         ((p)->p_sysent->sv_size == CGCOS_SYS_MAXSYSCALL))

extern pt_entry_t *pmap_pte(pmap_t pmap, vm_offset_t va);
#define pmap_pte_v(pte)    ((*(int *)pte & PG_V) != 0)
#define pmap_pte_rw(pte)   ((*(int *)pte & PG_RW) != 0)

//extern void pmap_pte_release(pt_entry_t *);

/*
 * copy data from hypervisor to guest-user process.  We cannot generate
 * a fault, so we must make sure all necessary pages are mapped.  If we
 * are missing any pages or if we're writing to readonly pages, inject
 * a page-fault to the guest.  Assuming the pages are valid, it will
 * map them and we'll end up here again (with mapped pages).
 */
static int
cgcvmx_copyout(struct proc *p, VmxStruct *vmx, uint16_t seg,
               uintptr_t off, void *ptr, int len) {
   pt_entry_t *pte = NULL;
   uint64_t nbytes;
   int result = 0;
   uint16_t host_es = RegGetEs();
   
   //kernel's copyout uses movs, source seg is DS, dest seg is ES
   RegSetEs(seg);   //setup destination segment

   while (len) {
      pte = pmap_pte(vm_map_pmap(&p->p_vmspace->vm_map), off);
      if (pte == NULL ||   // No PTE found ...
            !pmap_pte_v(pte) ||   // Found, but not valid...
            !pmap_pte_rw(pte)) {  // Found, but read-only...
         uint32_t code = PGEX_W | PGEX_U;
         
         if (pte && pmap_pte_v(pte) && !pmap_pte_rw(pte))
            code |= PGEX_P;

         vmx->cr2 = off;
         injectInterrupt(X86_PAGE_FAULT, INJECT_INTR_TYPE_HARDWARE_EXC,
         vm_read(VMX_EXIT_INSTR_LEN), 1, code);
         result = -1;
         goto out;
      }

      // Here, the current page (off) is mapped.  Copy some data to it.
      
      nbytes = PAGE_SIZE - (off & PAGE_MASK);
      if (nbytes > len)
         nbytes = len;
      result = copyout(ptr, (void *)off, nbytes);
      
      ptr = ((char *)ptr) + nbytes;
      off += nbytes;
      len -= nbytes; 
   }
   
out:
   RegSetEs(host_es);        //restore host ES
   return result ? -1 : 0;
}

static int cgcvmx_handle_gdt_idt(struct proc *p, VmxStruct *vmx) {
   uintptr_t iinfo, cs, r, offset;
   uint16_t seg;
   struct dtreg {
      uint16_t limit;
      uintptr_t base;
   } __attribute__((packed));
   struct dtreg dtr;

   iinfo = vm_read(VMX_INSTR_INFO);

   /* offset = disp + base + (index * scale) */
   offset = vm_read(VMX_EXIT_QUALIFICATION); /* disp */

   if ((iinfo & (1 << 27)) == 0) { /* base */
      if (cgcvmx_register_by_index((iinfo >> 23) & 0xf, vmx, &r)) {
         return (-1);
      }
      offset += r;
   }

   if ((iinfo & (1 << 22)) == 0) { /* index * scale */
      if (cgcvmx_register_by_index((iinfo >> 18) & 0xf, vmx, &r)) {
         return (-1);
      }
      r <<= iinfo & 3;
      offset += r;
   }

   switch (iinfo & 0x380) {
   case 0x000: /* 16 bit */
      offset &= 0xffff;
      break;
   case 0x080: /* 32 bit */
      offset &= 0xffffffff;
      break;
   case 0x100: /* 64 bit */
      break;
   default:
      injectInterrupt(X86_INVALID_OPCODE,
		      INJECT_INTR_TYPE_HARDWARE_EXC,
		      vm_read(VMX_EXIT_INSTR_LEN), 0, 0);
      return (-1);
   }
   
   cs = vm_read(VMX_GUEST_CS_SEL);

   if (cgcvmx_selector_by_index((iinfo >> 15) & 7, vmx, &seg))
      return (-1);

   switch (iinfo & 0x30000000) {
   case 0x00000000: /* SGDT */
      if ((cs & 3) == 3 && PROC_IS_CGC(p)) {
         /* lie: useland doesn't get the real SGDT */
         dtr.limit = 0xff;
         dtr.base = 0xf7beb000;
      }
      else {
         dtr.limit = vm_read(VMX_GUEST_GDTR_LIMIT);
         dtr.base = vm_read(VMX_GUEST_GDTR_BASE);
      }

      if ((iinfo & (1 << 11)) == 0) {
         /* 16 bit, truncate base to 24 bits */
         dtr.base &= 0xffffff;
      }

      if ((cs & 3) == 3) {
         if (cgcvmx_copyout(p, vmx, seg, offset, &dtr, sizeof(dtr))) {
            return (-1);
         }
      }
      else {
         //reached this from guest kernel space
         //but guest kernel == host kernel so we can just
         //do a segment aware memcpy
         //*** for now we lazily assume that we can get away 
         //    with a straight memcpy
         memcpy((void*)offset, &dtr, sizeof(dtr));
      }
      break;
   case 0x10000000: /* SIDT */
      if ((cs & 3) == 3 && PROC_IS_CGC(p)) {
         /* lie: useland doesn't get the real SIDT */
         dtr.limit = 0x7ff;
         dtr.base = 0xfffba000;
      }
      else {
         dtr.limit = vm_read(VMX_GUEST_GDTR_LIMIT);
         dtr.base = vm_read(VMX_GUEST_GDTR_BASE);
      }
      
      if ((iinfo & (1 << 11)) == 0) {
         /* 16 bit, truncate base to 24 bits */
         dtr.base &= 0xffffff;
      }

      if ((cs & 3) == 3) {
         if (cgcvmx_copyout(p, vmx, seg, offset, &dtr, sizeof(dtr))) {
            return (-1);
         }
      }
      else {
         //reached this from guest kernel space
         //but guest kernel == host kernel so we can just
         //do a segment aware memcpy
         //*** for now we lazily assume that we can get away 
         //    with a straight memcpy
         memcpy((void*)offset, &dtr, sizeof(dtr));
      }
      break;
   case 0x20000000: /* LGDT */
      if ((cs & 3) != 0) {
         injectInterrupt(X86_GENERAL_PROTECTION,
			 INJECT_INTR_TYPE_HARDWARE_EXC,
			 vm_read(VMX_EXIT_INSTR_LEN), 0, 0);
         return (-1);
      }

      //reached this from guest kernel space
      //but guest kernel == host kernel so we can just
      //do a segment aware memcpy
      //*** for now we lazily assume that we can get away 
      //    with a straight memcpy
      memcpy(&dtr, (void*)offset, sizeof(dtr));

      cgc_vmwrite(VMX_GUEST_GDTR_LIMIT, dtr.limit);
      cgc_vmwrite(VMX_GUEST_GDTR_BASE, dtr.base);
      break;

   case 0x30000000: /* LIDT */
      if ((cs & 3) != 0) {
         injectInterrupt(X86_GENERAL_PROTECTION,
			 INJECT_INTR_TYPE_HARDWARE_EXC,
			 vm_read(VMX_EXIT_INSTR_LEN), 0, 0);
         return (-1);
      }

      //reached this from guest kernel space
      //but guest kernel == host kernel so we can just
      //do a segment aware memcpy
      //*** for now we lazily assume that we can get away 
      //    with a straight memcpy
      memcpy(&dtr, (void*)offset, sizeof(dtr));
      
      cgc_vmwrite(VMX_GUEST_IDTR_LIMIT, dtr.limit);
      cgc_vmwrite(VMX_GUEST_IDTR_BASE, dtr.base);
      break;
   }

   return (0);
}

static int cgcvmx_handle_ldt_tr(struct proc *p, VmxStruct *vmx) {
   uintptr_t iinfo, base, limit, arb, r, offset; //, x_iinfo, x_exitq;
   uint16_t usel, cs, seg;
   struct {
      uintptr_t seg1;
      uintptr_t seg2;
   } segdesc;

   /*
    * instruction info field:
    * 0:1 scaling
    *     0 - no scaling
    *     1 - scale by 2
    *     2 - scale by 4
    *     3 - scale by 8 (only on Intel 64 arch)
    * 6:3 register
    *     0 rax, 1 rcx, 2 rdx, 3 rbx, 4 rsp, 5 rbp, 6 rsi, 7 rdi, 8-15 r8-15
    * 9:7 address size
    *     0 - 16 bit
    *     1 - 32 bit
    *     2 - 64 bit (only on Intel 64 arch)
    * 10 mem(0) or register(1)
    * 17:15 segment register
    *     0 es, 1 cs, 2 ss, 3 ds, 4 fs, 5 gs
    * 21:18 index register (encoded like 6:3)
    *     undef for register instructions (bit 10 is set) and memory
    *     instructions with no index register (bit 10 clear, bit 22 set)
    * 22 index register invalid (0=valid, 1=invalid)
    * 26:23 base register (encoded like 6:3)
    *     undef for register instructions (bit 10 is set) and memory
    *     instructions with no base register (bit 10 clear, bit 27 set)
    * 27 base register invalid (0=valid, 1=invalid)
    * 29:28 instruction
    *     0 sldt, 1 str, 2 lldt, 3 ltr
    */
   //x_iinfo = iinfo = vm_read(VMX_INSTR_INFO);
   iinfo = vm_read(VMX_INSTR_INFO);

   if ((iinfo & (1 << 10)) == 0) {
      /* memory: offset = disp + base + (index * scale) */

      /* disp */
      //x_exitq = offset = vm_read(VMX_EXIT_QUALIFICATION);
      offset = vm_read(VMX_EXIT_QUALIFICATION);

      /* base */
      if ((iinfo & (1 << 27)) == 0) {
         if (cgcvmx_register_by_index((iinfo >> 23) & 0xf, vmx, &r))
            return (-1);
         offset += r;
      }

      /* index*scale */
      if ((iinfo & (1 << 22)) == 0) {
         if (cgcvmx_register_by_index((iinfo >> 18) & 0xf, vmx, &r))
            return (-1);
         r <<= iinfo & 3;
         offset += r;
      }

      switch (iinfo & 0x380) {
      case 0x000: /* 16 bit */
         offset &= 0xffff;
         break;
      case 0x080: /* 32 bit */
         offset &= 0xffffffff;
         break;
      case 0x100: /* 64 bit */
         break;
      default:
         injectInterrupt(X86_INVALID_OPCODE,
			 INJECT_INTR_TYPE_HARDWARE_EXC,
			 vm_read(VMX_EXIT_INSTR_LEN), 0, 0);
	 return (-1);
      }
   } else
      offset = 0;

   cs = vm_read(VMX_GUEST_CS_SEL);

   switch (iinfo & 0x30000000) {
   case 0x00000000: /* SLDT */
      if ((cs & 3) == 3 && PROC_IS_CGC(p)) {
         /* lie: userland doesn't get real LDT */
         usel = 0;
      }
      else {
         usel = vm_read(VMX_GUEST_LDTR_SEL);
      }
      if ((iinfo & (1 << 10)) == 0) {
         if (cgcvmx_selector_by_index((iinfo >> 15) & 7, vmx, &seg)) {
            return (-1);
         }
         if ((cs & 3) == 3) {
            if (cgcvmx_copyout(p, vmx, seg, offset, &usel, sizeof(usel))) {
               injectInterrupt(X86_PAGE_FAULT, INJECT_INTR_TYPE_HARDWARE_EXC,
               vm_read(VMX_EXIT_INSTR_LEN), 1, 6);
               return (-1);
            }
         }
         else {
            //reached this from guest kernel space
            //but guest kernel == host kernel so we can just
            //do a segment aware memcpy
            //*** for now we lazily assume that we can get away 
            //    with a straight memcpy
            memcpy((void*)offset, &usel, sizeof(usel));
         }
      }
      else {
         if (cgcvmx_register_by_index((iinfo >> 3) & 0xf, vmx, &r)) {
            return (-1);
         }
         r &= ~0xffff;
         r |= usel;
         if (cgcvmx_register_by_index_put((iinfo >> 3) & 0xf, vmx, r)) {
            return (-1);
         }
      }
      break;
   case 0x10000000: /* STR */
      if ((cs & 3) == 3 && PROC_IS_CGC(p)) {
         /* lie: userland doesn't get real TR */
         usel = 0x80;
      }
      else {
         usel = vm_read(VMX_GUEST_TR_SEL);
      }
      if ((iinfo & (1 << 10)) == 0) {
         if (cgcvmx_selector_by_index((iinfo >> 15) & 7, vmx, &seg)) {
            return (-1);
         }
         if ((cs & 3) == 3) {
            if (cgcvmx_copyout(p, vmx, seg, offset, &usel, sizeof(usel))) {
               return (-1);
            }
         }
         else {
            //reached this from guest kernel space
            //but guest kernel == host kernel so we can just
            //do a segment aware memcpy
            //*** for now we lazily assume that we can get away 
            //    with a straight memcpy
            memcpy((void*)offset, &usel, sizeof(usel));
         }
      }
      else {
         if (cgcvmx_register_by_index((iinfo >> 3) & 0xf, vmx, &r)) {
            return (-1);
         }
         r &= ~0xffff;
         r |= usel;
         if (cgcvmx_register_by_index_put((iinfo >> 3) & 0xf, vmx, r)) {
            return (-1);
         }
      }
      break;
   case 0x20000000: /* LLDT */
      if ((cs & 3) != 0) {
         injectInterrupt(X86_GENERAL_PROTECTION,
                        INJECT_INTR_TYPE_HARDWARE_EXC,
                        vm_read(VMX_EXIT_INSTR_LEN), 0, 0);
         return (-1);
      }
      
      //*** cs == 0 ==> we're here from guest kernel space

      if ((iinfo & (1 << 10)) == 0) {
         if (cgcvmx_selector_by_index((iinfo >> 15) & 7, vmx, &seg)) {
            return (-1);
         }
         //reached this from guest kernel space
         //but guest kernel == host kernel so we can just
         //do a segment aware memcpy
         //*** for now we lazily assume that we can get away 
         //    with a straight memcpy
         memcpy(&usel, (void*)offset, sizeof(usel));
      }
      else {
         if (cgcvmx_register_by_index((iinfo >> 3) & 0xf, vmx, &r)) {
            return (-1);
         }
         usel = r;
      }

      if (usel & 4) {
         /* can't be in LDT */
         injectInterrupt(X86_GENERAL_PROTECTION,
             INJECT_INTR_TYPE_HARDWARE_EXC,
             vm_read(VMX_EXIT_INSTR_LEN), 0, 0);
         return (-1);
      }

      limit = vm_read(VMX_GUEST_GDTR_LIMIT);
      if ((usel | 7) > limit) {
         /* past limit */
         injectInterrupt(X86_GENERAL_PROTECTION,
             INJECT_INTR_TYPE_HARDWARE_EXC,
             vm_read(VMX_EXIT_INSTR_LEN), 1, usel);
         return (-1);
      }

      if ((usel & 0xfffc0) == 0) {
         base = 0;
         usel = 0;
         limit = -1;
         arb = 0x1c000;
      }
      else {
         base = vm_read(VMX_GUEST_GDTR_BASE);

         //reached this from guest kernel space
         //but guest kernel == host kernel so we can just
         //do a memcpy
         //*** for now we lazily assume that we can get away 
         //    with a straight memcpy
         memcpy(&segdesc, (void*)(base + (usel & (~7))), sizeof(segdesc));

         if ((segdesc.seg2 & 0x1f00) != 0x0200) {
            /* segment is not for an LDT */
            injectInterrupt(X86_GENERAL_PROTECTION,
                INJECT_INTR_TYPE_HARDWARE_EXC,
                vm_read(VMX_EXIT_INSTR_LEN), 1, usel);
            return (-1);
         }

         if ((segdesc.seg2 & 0x8000) == 0) {
            /* descriptor not present */
            injectInterrupt(X86_SEG_NOT_PRESENT,
                INJECT_INTR_TYPE_HARDWARE_EXC,
                vm_read(VMX_EXIT_INSTR_LEN), 1, usel);
            return (-1);
         }

         limit = (segdesc.seg1 & 0xffff) | (segdesc.seg2 & 0x000f0000);
         base = (segdesc.seg1 >> 16) | (segdesc.seg2 & 0xff000000) |
                ((segdesc.seg2 & 0xff) << 16);
         arb = ((segdesc.seg2 & 0xff00) >> 8) |
               ((segdesc.seg2 & 0xf00000) >> 8);
      }

      cgc_vmwrite(VMX_GUEST_LDTR_ATTR, arb);
      cgc_vmwrite(VMX_GUEST_LDTR_BASE, base);
      cgc_vmwrite(VMX_GUEST_LDTR_LIMIT, limit);
      cgc_vmwrite(VMX_GUEST_LDTR_SEL, usel);
      break;
   case 0x30000000: /* LTR */
      if ((cs & 3) != 0) {
         injectInterrupt(X86_GENERAL_PROTECTION,
                        INJECT_INTR_TYPE_HARDWARE_EXC,
                        vm_read(VMX_EXIT_INSTR_LEN), 0, 0);
         return (-1);
      }

      //*** cs == 0 ==> reached this from guest kernel

      if ((iinfo & (1 << 10)) == 0) {
         if (cgcvmx_selector_by_index((iinfo >> 15) & 7, vmx, &seg)) {
            return (-1);
         }
         //reached this from guest kernel space
         //but guest kernel == host kernel so we can just
         //do a segment aware memcpy
         //*** for now we lazily assume that we can get away 
         //    with a straight memcpy
         memcpy(&usel, (void*)offset, sizeof(usel));
      }
      else {
         if (cgcvmx_register_by_index((iinfo >> 3) & 0xf, vmx, &r)) {
            return (-1);
         }
         usel = r;
      }

      if ((usel & 0xfffc) == 0) {
         /* NULL selector */
         injectInterrupt(X86_GENERAL_PROTECTION,
                        INJECT_INTR_TYPE_HARDWARE_EXC,
                        vm_read(VMX_EXIT_INSTR_LEN), 0, 0);
         return (-1);
      }

      if (usel & 4) {
         /* can't be in LDT */
         injectInterrupt(X86_GENERAL_PROTECTION,
             INJECT_INTR_TYPE_HARDWARE_EXC,
             vm_read(VMX_EXIT_INSTR_LEN), 1, usel);
         return (-1);
      }

      limit = vm_read(VMX_GUEST_GDTR_LIMIT);
      if ((usel | 7) > limit) {
         /* past limit */
         injectInterrupt(X86_GENERAL_PROTECTION,
             INJECT_INTR_TYPE_HARDWARE_EXC,
             vm_read(VMX_EXIT_INSTR_LEN), 1, usel);
         return (-1);
      }

      base = vm_read(VMX_GUEST_GDTR_BASE);

      //reached this from guest kernel space
      //but guest kernel == host kernel so we can just
      //do a memcpy
      //*** for now we lazily assume that we can get away 
      //    with a straight memcpy
      memcpy(&segdesc, (void*)(base + (usel & (~7))), sizeof(segdesc));

      if ((segdesc.seg2 & 0x1f00) != 0x0900) {
         /* segment is not for an available TSS */
         injectInterrupt(X86_GENERAL_PROTECTION,
             INJECT_INTR_TYPE_HARDWARE_EXC,
             vm_read(VMX_EXIT_INSTR_LEN), 1, usel);
         return (-1);
      }

      if ((segdesc.seg2 & 0x8000) == 0) {
         /* descriptor not present */
         injectInterrupt(X86_SEG_NOT_PRESENT,
             INJECT_INTR_TYPE_HARDWARE_EXC,
             vm_read(VMX_EXIT_INSTR_LEN), 1, usel);
         return (-1);
      }

      limit = (segdesc.seg1 & 0xffff) | (segdesc.seg2 & 0x000f0000);
      base = (segdesc.seg1 >> 16) | (segdesc.seg2 & 0xff000000) |
            ((segdesc.seg2 & 0xff) << 16);
      arb = ((segdesc.seg2 & 0xff00) >> 8) |
            ((segdesc.seg2 & 0xf00000) >> 8);
      arb |= 2; /* set to BUSY */

      cgc_vmwrite(VMX_GUEST_TR_ATTR, arb);
      cgc_vmwrite(VMX_GUEST_TR_BASE, base);
      cgc_vmwrite(VMX_GUEST_TR_LIMIT, limit);
      cgc_vmwrite(VMX_GUEST_TR_SEL, usel);
      break;
   }
   return (0);
}

#define CPUID_1_ECX_RDRAND   (1 << 30)   /* rdrand */
#define CPUID_1_ECX_OSXSAVE  (1 << 27)   /* os xsave support */
#define CPUID_1_ECX_XSAVE    (1 << 26)   /* xsave */
#define CPUID_1_EDX_SEP      (1 << 11)   /* sysenter/sysexit */
#define CPUID_1_EDX_TSC      (1 << 4)    /* time stamp counters */

struct cpuids {
   uint32_t eax;
   uint32_t ebx;
   uint32_t ecx;
   uint32_t edx;
};
static struct cpuids cpuid_basic[] = {
   { 0x00000005, 0x756e6547, 0x6c65746e, 0x49656e69 },
   { 0x00040651, 0x00000800, 0x00000209, 0x078bfbff },
   { 0x76036301, 0x00f0b5ff, 0x00000000, 0x00c10000 },
   { 0x00000000, 0x00000000, 0x00000000, 0x00000000 },
   { 0x00000000, 0x00000000, 0x00000000, 0x00000000 }, /* special */
   { 0x00000040, 0x00000040, 0x00000000, 0x00000000 }
};

static struct cpuids cpuid_ext[] = {
   { 0x80000008, 0x00000000, 0x00000000, 0x00000000 },
   { 0x00000000, 0x00000000, 0x00000001, 0x28100800 },
   { 0x65746e49, 0x2952286c, 0x726f4320, 0x4d542865 },
   { 0x35692029, 0x3532342d, 0x43205538, 0x40205550 },
   { 0x342e3220, 0x7a484730, 0x00000000, 0x00000000 },
   { 0x00000000, 0x00000000, 0x00000000, 0x00000000 },
   { 0x00000000, 0x00000000, 0x01006040, 0x00000000 },
   { 0x00000000, 0x00000000, 0x00000000, 0x00000000 },
   { 0x00003027, 0x00000000, 0x00000000, 0x00000000 }
};

static struct cpuids cpuid_dfl = {
   0x00000007, 0x00000340, 0x00000340, 0x00000000
};

struct cpuids cpuid_leaf4[] = {
   { 0x00000021, 0x01c0003f, 0x0000003f, 0x00000000 },
   { 0x00000021, 0x01c0003f, 0x0000003f, 0x00000000 },
   { 0x00000041, 0x05c0003f, 0x00000fff, 0x00000000 },
};

#define N(t) (sizeof(t) / sizeof(t[0]))

static void
cgcvmx_cpuid(struct proc *p, VmxStruct *vmx, CpuIdRegs *regs) {
   // vmx has the entry registers, regs has the exit registers
   struct cpuids *leaf;

   if (p == NULL || p->p_sysent->sv_size != CGCOS_SYS_MAXSYSCALL) {
      //test for user space and specific personality
      //sysentvec.sv_size is a pretty lame check
      //may need to use sysentvec.sv_name
      return;
   }

#if 0
   printf("<cgcvmx> VM exit for CPUID, in pid %d, cpu %u\n", p->p_pid, vmx->procNum);
#endif

   // Don't tell me this is wrong.  This is what vbox does. */
   if (LO32(vmx->rax) == 4 && LO32(vmx->rcx) < N(cpuid_leaf4))
      leaf = &cpuid_leaf4[LO32(vmx->rcx)];
   else if (LO32(vmx->rax) < N(cpuid_basic))
      leaf = &cpuid_basic[LO32(vmx->rax)];
   else if (LO32(vmx->rax) - 0x80000000 < N(cpuid_ext))
      leaf = &cpuid_ext[LO32(vmx->rax) - 0x80000000];
   else
      leaf = &cpuid_dfl;

   regs->eax = leaf->eax;
   regs->ebx = leaf->ebx;
   regs->ecx = leaf->ecx;
   regs->edx = leaf->edx;
}

static int
do_io_inb(VmxStruct *vmx, uint16_t port) {
   uint8_t r;
   
   __asm__ volatile("inb %w1, %b0" : "=a"(r) : "d"(port));
   vmx->rax &= ~0xff;
   vmx->rax |= r & 0xff;
   return (0);
}

static int
do_io_inw(VmxStruct *vmx, uint16_t port) {
   uint16_t r;
   
   __asm__ volatile("inw %w1, %w0" : "=a"(r) : "d"(port));
   vmx->rax &= ~0xffff;
   vmx->rax |= r & 0xffff;
   return (0);
}

static int
do_io_inl(VmxStruct *vmx, uint16_t port) {
   uint32_t r;
   
   __asm__ volatile("inl %w1, %0" : "=a"(r) : "d"(port));
   vmx->rax &= ~0xffffffffLL;
   vmx->rax |= r;
   return (0);
}

static int
do_io_outb(VmxStruct *vmx, uint16_t port) {
   __asm__ volatile("outb %b0, %w1" : : "a"(vmx->rax), "d"(port));
   return (0);
}

static int
do_io_outw(VmxStruct *vmx, uint16_t port) {
   __asm__ volatile("outw %w0, %w1" : : "a"(vmx->rax), "d"(port));
   return (0);
}

static int
do_io_outl(VmxStruct *vmx, uint16_t port) {
   __asm__ volatile("outl %k0, %w1" : : "a"(vmx->rax), "d"(port));
   return (0);
}

static int
do_io_insb(VmxStruct *vmx, uint16_t port) {
   __asm__ volatile("insb": "+D"(vmx->rdi) : "d"(port));
   return (0);
}

static int
do_io_insw(VmxStruct *vmx, uint16_t port) {
   __asm__ volatile("insw": "+D"(vmx->rdi) : "d"(port));
   return (0);
}

static int
do_io_insl(VmxStruct *vmx, uint16_t port) {
   __asm__ volatile("insl": "+D"(vmx->rdi) : "d"(port));
   return (0);
}

static int
do_io_outsb(VmxStruct *vmx, uint16_t port) {
   __asm__ volatile("outsb": "+S"(vmx->rsi) : "d"(port));
   return (0);
}

static int
do_io_outsw(VmxStruct *vmx, uint16_t port) {
   __asm__ volatile("outsw": "+S"(vmx->rsi) : "d"(port));
   return (0);
}

static int
do_io_outsl(VmxStruct *vmx, uint16_t port) {
   __asm__ volatile("outsl": "+S"(vmx->rsi) : "d"(port));
   return (0);
}

static int
do_io_rep_insb(VmxStruct *vmx, uint16_t port) {
   __asm__ volatile("rep; insb": "+D"(vmx->rdi), "+c"(vmx->rcx) : "d"(port));
   return (0);
}

static int
do_io_rep_insw(VmxStruct *vmx, uint16_t port) {
   __asm__ volatile("rep; insw": "+D"(vmx->rdi), "+c"(vmx->rcx) : "d"(port));
   return (0);
}
static int
do_io_rep_insl(VmxStruct *vmx, uint16_t port) {
   __asm__ volatile("rep; insl": "+D"(vmx->rdi), "+c"(vmx->rcx) : "d"(port));
   return (0);
}

static int
do_io_rep_outsb(VmxStruct *vmx, uint16_t port) {
   __asm__ volatile("rep; outsb": "+S"(vmx->rsi), "+c"(vmx->rcx) : "d"(port));
   return (0);
}

static int
do_io_rep_outsw(VmxStruct *vmx, uint16_t port) {
   __asm__ volatile("rep; outsw": "+S"(vmx->rsi), "+c"(vmx->rcx) : "d"(port));
   return (0);
}

static int
do_io_rep_outsl(VmxStruct *vmx, uint16_t port) {
   __asm__ volatile("rep; outsl": "+S"(vmx->rsi), "+c"(vmx->rcx) : "d"(port));
   return (0);
}

static int (*do_io[64])(VmxStruct *vmx, uint16_t port) = {
   /* basic i/o instructions */
   do_io_outb, do_io_outw, NULL, do_io_outl,
   NULL, NULL, NULL, NULL,
   do_io_inb, do_io_inw, NULL, do_io_inl,
   NULL, NULL, NULL, NULL,

   /* string i/o instructions */
   do_io_outsb, do_io_outsw, NULL, do_io_outsl,
   NULL, NULL, NULL, NULL,
   do_io_insb, do_io_insw, NULL, do_io_insl,
   NULL, NULL, NULL, NULL,


   /* REP + basic i/o instructions (invalid) */
   NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL,
   NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL,

   /* REP + string i/o instructions (e.g. rep ins, rep outs) */
   do_io_rep_outsb, do_io_rep_outsw, NULL, do_io_rep_outsl,
   NULL, NULL, NULL, NULL,
   do_io_rep_insb, do_io_rep_insw, NULL, do_io_rep_insl,
   NULL, NULL, NULL, NULL,
};

static int
cgcvmx_handle_io(struct proc *p, VmxStruct *vmx) {
   uintptr_t qual; //iinfo, qual;
   uint16_t dx, cs;
   int idx;
   
   cs = vm_read(VMX_GUEST_CS_SEL);
   if ((cs & 3) != 0) {
      /* here's a big middle finger to userland (correct would be to check
       * the IOPL and TSS)...
       */
      injectInterrupt(X86_GENERAL_PROTECTION,
            INJECT_INTR_TYPE_HARDWARE_EXC,
            vm_read(VMX_EXIT_INSTR_LEN), 0, 0);
      return (-1);
   }

   /*
    * at this point we have to emulate the instruction on behalf of the
    * kernel, so we can assume the segment registers are correct, but
    * we need guest rsi/rdi/rcx.
    */

   /* TODO: with move to 64-bit O/S I think we need to 
      factor in address size field (9:7) from 
      VMX_INSTR_INFO field to choose rcx/ecx/cx
      as appropriate */
   //iinfo = vm_read(VMX_INSTR_INFO);
   qual = vm_read(VMX_EXIT_QUALIFICATION);
   
   dx = (qual >> 16) & 0xffff;
   idx = qual & 0x3f;

//   printf("io cpl=%d qual=0x%x(port=%x, idx=%x)!\n", cs & 3, qual, dx, idx);
   
   if (do_io[idx] == NULL) {
      /* Uh oh, I haven't implemented these yet. */
      printf("<cgcvmx> unimplemented i/o insn (0x%x)\n", idx);
      injectInterrupt(X86_GENERAL_PROTECTION,
            INJECT_INTR_TYPE_HARDWARE_EXC,
            vm_read(VMX_EXIT_INSTR_LEN), 0, 0);
      return (-1);
   }

   do_io[idx](vmx, dx);
   return (0);
}

//handle all vm exits here
void vmexitCallback(VmxStruct *vmx) {
   uint32_t user_cs;
   uint32_t inst_len;
   uintptr_t rflags;
   uint32_t inst_info;
   uint64_t qual;
   uint32_t reason;
   uintptr_t guest_rip;
   uint16_t guest_cs;

   struct proc *p = NULL;
   struct thread *td = NULL;

   if (vmx == NULL) {
      return;
   }

   //guest rsp is not saved when vmx state is setup because rsp already
   //points to host stack, so fetch saved guest rsp
   vmx->rsp = vm_read(VMX_GUEST_RSP);

   inst_len = (uint32_t)vm_read(VMX_EXIT_INSTR_LEN);
   guest_cs = (uint16_t)vm_read(VMX_GUEST_CS_SEL);

   user_cs = (guest_cs & 3) == 3;
   if (user_cs) {
      td = curthread;
      p = td->td_proc;
   }

   guest_rip = vm_read(VMX_GUEST_RIP);
   inst_info = (uint32_t)vm_read(VMX_INSTR_INFO);
   reason = (uint32_t)vm_read(VMX_EXIT_REASON);
   rflags = vm_read(VMX_GUEST_RFLAGS);
   qual = vm_read(VMX_EXIT_QUALIFICATION);
   vmx->cr2 = RegGetCr2();

   if ((reason & VMEXIT_REASONS_FAILED_VMENTRY) != 0) {
      //at this point we can't continue. we need to back out of this
      //mess, deallocating our resources and doing a vmxoff
      printf("<cgcvmx> Failed VM entry: reason 0x%08x\n", reason);
   }

   switch (reason & 0xffff) {
      case VMEXIT_REASON_INIT:
	 if (vmx->procNum == 0) {
	    extern void cpu_reset_real(void);

	    disable_intr();
	    cgc_vmxoff(vmx->procNum);
	    cpu_reset_real();
	 } else {
	    disable_intr();
	    for (;;)
	       ia32_pause();
	 }
	 break;
      case VMEXIT_REASON_CPUID: {
         //cpuid generates unconditional vm exit
         //flags not affected
         CpuIdRegs regs;
         regs.eax =(uint32_t) vmx->rax;
         regs.ecx = (uint32_t)vmx->rcx;
         cgc_cpuid(&regs);
         cgcvmx_cpuid(p, vmx, &regs);
         vmx->rax = regs.eax;
         vmx->rbx = regs.ebx;
         vmx->rcx = regs.ecx;
         vmx->rdx = regs.edx;
         break;
      }
      case VMEXIT_REASON_HLT:
         //cgc_vmwrite(VMX_GUEST_ACTIVITY_STATE, 1);
         break;
      case VMEXIT_REASON_VMCALL:
         vmx->rax = hypercall_md((void *)vm_read(VMX_GUEST_RIP), vmx->rbx, vmx->rdx, vmx->r8);
         break;
      case VMEXIT_REASON_CR_ACCESS: {
         uint64_t reg = qual & 0xf;
         uint32_t type = (qual >> 4) & 3;
         uint64_t gpr = (qual >> 8) & 0xf;
         switch (type) {
            case 0: {  // mov to cr
               //rflags undefined
               uint64_t val = (&vmx->rax)[-gpr];
               switch (reg) {
                  case 0:
                     cgc_vmwrite(VMX_GUEST_CR0, val);
//                     RegSetCr0(val);
                     break;
                  case 3:
                     cgc_vmwrite(VMX_GUEST_CR3, val);
		     cgc_vmwrite(VMX_HOST_CR3, val);
                     RegSetCr3(val);
                     break;
                  case 4:
                     cgc_vmwrite(VMX_GUEST_CR4, val);
//                     RegSetCr4(val);
                     break;
                  default:
                     //should never get here
                     break;
               }
               //printf("<cgcvmx> VM exit for mov cr%u, 0x%08x\n", reg, val);
               break;
            }
            case 1: {  // mov from cr
               //rflags undefined
               switch (reg) {
                  case 0:
                     (&vmx->rax)[-gpr] = vm_read(VMX_GUEST_CR0);
                     break;
                  case 3:
                     (&vmx->rax)[-gpr] = vm_read(VMX_GUEST_CR3);
                     break;
                  case 4:
                     (&vmx->rax)[-gpr] = vm_read(VMX_GUEST_CR4);
                     break;
                  default:
                     //should never get here
                     break;
               }
               //printf("<cgcvmx> VM exit for mov r%u, cr%u\n", gpr, reg);
               break;
            }
            case 2:   // clts
               //rflags unaffected
               cgc_vmwrite(VMX_GUEST_CR0, vm_read(VMX_GUEST_CR0) & ~CR0_TASK_SWITCHED_FLAG);
//               cgc_clts();
               //printf("<cgcvmx> VM exit for clts\n");
               break;
            case 3:   // lmsw
               //rflags unaffected
               //emulate loading low 4 bits of CR0 from operand
               cgc_vmwrite(VMX_GUEST_CR0, (vm_read(VMX_GUEST_CR0) & ~0xf) | ((qual >> 16) & 0xf));
               //cgc_lmsw(qual >> 16);
               //printf("<cgcvmx> VM exit for lmsw 0x%08x\n", qual >> 16);
               break;
         }
         break;
      }
      case VMEXIT_REASON_RDTSC:
         //The RDTSC instruction causes a VM exit if the "RDTSC exiting" VM-execution control is 1.
         //no flags affected
         if (!PROC_IS_CGC(p)) {
            //if it's kernel code or not CGC, they can see tsc
            uint64_t tsc = RegGetTSC();
            vmx->rax = tsc & 0xffffffff;
            vmx->rdx = (tsc >> 32) & 0xffffffff;
         }
         else {
            //printf("<cgcvmx> VM exit for RDTSC, in pid %d, cpu %u\n", p->p_pid, vmx->procNum);
            //deliver #UD
            injectInterrupt(X86_INVALID_OPCODE, INJECT_INTR_TYPE_HARDWARE_EXC, inst_len, 0, 0);
            goto exceptout;
         }
         break;
      case VMEXIT_REASON_RDRAND:
      case VMEXIT_REASON_RDSEED:
         //The RDRAND/RDSEED instructions cause a VM exit if the "RDRAND/RDSEED exiting" VM-execution control is 1.
         //inst_info contains dest reg in 6:3, operand size in 12:11
         //rdrand/rdseed set CF = 1 on success
         if (!PROC_IS_CGC(p)) {
            //let the kernel and non-CGC access rdrand/rdseed
            uint64_t rnd;
            uint64_t reg = (inst_info >> 3) & 0xf;
            uint32_t carry;
            if ((reason & 0xffff) == VMEXIT_REASON_RDRAND)
               carry = cgc_rdrand(&rnd) ? CARRY_FLAG : 0;
            else
               carry = cgc_rdseed(&rnd) ? CARRY_FLAG : 0;

            modifyReg(&(&vmx->rax)[-reg], rnd, ((inst_info >> 11) & 3) + 1);
            cgc_vmwrite(VMX_GUEST_RFLAGS, (rflags  & ~CARRY_FLAG) | carry);
         }
         else {
            //deliver #UD
            //printf("<cgcvmx> VM exit for rdrand/rdseed\n");
            injectInterrupt(X86_INVALID_OPCODE, INJECT_INTR_TYPE_HARDWARE_EXC, inst_len, 0, 0);
            goto exceptout;
         }
         break;

      case VMEXIT_REASON_MSR_READ:
         vmexit_rdmsr(vmx);
         break;

      case VMEXIT_REASON_MSR_WRITE:
         vmexit_wrmsr(vmx);
         break;

      case VMEXIT_REASON_IO_INSTRUCTION:
         if (cgcvmx_handle_io(p, vmx)) {
            /* don't advance RIP on error  */
            goto exceptout;
         }
         break;

      case VMEXIT_REASON_ACCESS_GDTR_IDTR: //LGDT, LIDT, SGDT, or SIDT
         if (cgcvmx_handle_gdt_idt(p, vmx)) {
            /* don't advance RIP on error  */
            goto exceptout;
         }
         break;
      case VMEXIT_REASON_ACCESS_LDTR_TR:   //LLDT, LTR, SLDT, or STR
         if (cgcvmx_handle_ldt_tr(p, vmx)) {
            /* don't advance RIP on error */
            goto exceptout;
         }
         break;
      default:
         printf("<cgcvmx> cpu%d VM exit for reason 0x%08x\n", vmx->procNum, reason);
         break;
   }

   //may need to adjust guest rsp based on handler code
   cgc_vmwrite(VMX_GUEST_RSP, vmx->rsp);
   //advance rip
   cgc_vmwrite(VMX_GUEST_RIP, guest_rip + inst_len);

exceptout:
   RegSetCr2(vmx->cr2);
}

//adjust bits field with allowed ones and zeros to
//allow vm entry to succeed
static uint32_t computeMsrValue(uint32_t bits, uint32_t msr) {
   uint64_t value;
   value = MsrRead(msr);
//   printf("<cgcvmx> rdmsr(0x%x) = 0x%llx\n", msr, value);
   
   bits &= (uint32_t)(value >> 32);  /* bits 63:32 contain allowed 1 settings */
                                   /* if bit is a 0 in here it must be 0 on vm entry or entry will fail */
   bits |= (uint32_t)value;          /* bits 31:0 contain allowed 0 settings  */
   return bits;
}

static int
one_setting_allowed(uint32_t msr, uint32_t bit) {
   return ((MsrRead(msr) >> 32) & bit);
}

static void setup_control_16(void) {
   if (one_setting_allowed(MSR_IA32_VMX_PROCBASED_CTLS2,
			   CPU2_BASED_ENABLE_VPID))
      cgc_vmwrite(VMX_VPID, 0);

   if (one_setting_allowed(MSR_IA32_VMX_PINBASED_CTLS,
			   PIN_BASED_POSTED_INTRS))
      cgc_vmwrite(VMX_PI_NOTIFICATION_VEC, 0);

   if (one_setting_allowed(MSR_IA32_VMX_PROCBASED_CTLS2,
			   CPU2_BASED_EPT_VIOLATION))
      cgc_vmwrite(VMX_EPTP_INDEX, 0);
}

static void
cgc_pmcmsr_config(void) {
   CpuIdRegs regs;
   int i;

   regs.eax = 0;
   regs.ecx = 0;
   cgc_cpuid(&regs);

   if (regs.eax < 0xa) {
      printf("<cgcvmx> NO PMC PRESENT\n");
      return;
   }

   regs.eax = 0xa;
   regs.ecx = 0;
   cgc_cpuid(&regs);

   if (cpu_feature2 & CPUID2_PDCM)
      set_msr_bitmap(MSR_IA32_PERF_CAPABILITIES, 0, 0);
   
   // allow guest direct access to general purpose counters
   for (i = 0; i < ((regs.eax >> 8) & 0xff); i++) {
      set_msr_bitmap(MSR_IA32_PMC(i), 0, 0);
      set_msr_bitmap(MSR_IA32_A_PMC(i), 0, 0);
      set_msr_bitmap(MSR_IA32_PERFEVTSEL(i), 0, 0);
   }

   // if present, allow guest direct access to fixed counters
   // and global configuration
   if ((regs.eax & 0xff) > 1) {
      for (i = 0; i < ((regs.edx >> 0) & 0x1f); i++) {
         set_msr_bitmap(MSR_IA32_FIXED_CTR(i), 0, 0);
      }
      set_msr_bitmap(MSR_IA32_FIXED_CTR_CTRL, 0, 0);
      set_msr_bitmap(MSR_IA32_PERF_GLOBAL_CTRL, 0, 1);
      set_msr_bitmap(MSR_IA32_PERF_GLOBAL_STATUS, 0, 0);
      set_msr_bitmap(MSR_IA32_PERF_GLOBAL_OVFCTRL, 0, 0);
   }

   /* pass through some uncore PMC goop */

   set_msr_bitmap(MSR_UNCORE_PERF_GLOBAL_CTRL, 0, 0);
   set_msr_bitmap(MSR_UNCORE_PERF_GLOBAL_STATUS, 0, 0);
   set_msr_bitmap(MSR_UNCORE_PERF_GLOBAL_OVF_CTRL, 0, 0);
   set_msr_bitmap(MSR_UNCORE_PERF_FIXED_CTR0, 0, 0);
   set_msr_bitmap(MSR_UNCORE_PERF_FIXED_CTR_CTRL, 0, 0);
   set_msr_bitmap(MSR_UNCORE_ADDR_OPCODE_MATCH, 0, 0);

   set_msr_bitmap(MSR_UNCORE_PMC0, 0, 0);
   set_msr_bitmap(MSR_UNCORE_PMC1, 0, 0);
   set_msr_bitmap(MSR_UNCORE_PMC2, 0, 0);
   set_msr_bitmap(MSR_UNCORE_PMC3, 0, 0);
   set_msr_bitmap(MSR_UNCORE_PMC4, 0, 0);
   set_msr_bitmap(MSR_UNCORE_PMC5, 0, 0);
   set_msr_bitmap(MSR_UNCORE_PMC6, 0, 0);
   set_msr_bitmap(MSR_UNCORE_PMC7, 0, 0);

   set_msr_bitmap(MSR_UNCORE_PERFEVTSEL0, 0, 0);
   set_msr_bitmap(MSR_UNCORE_PERFEVTSEL1, 0, 0);
   set_msr_bitmap(MSR_UNCORE_PERFEVTSEL2, 0, 0);
   set_msr_bitmap(MSR_UNCORE_PERFEVTSEL3, 0, 0);
   set_msr_bitmap(MSR_UNCORE_PERFEVTSEL4, 0, 0);
   set_msr_bitmap(MSR_UNCORE_PERFEVTSEL5, 0, 0);
   set_msr_bitmap(MSR_UNCORE_PERFEVTSEL6, 0, 0);
   set_msr_bitmap(MSR_UNCORE_PERFEVTSEL7, 0, 0);

   set_msr_bitmap(MSR_UNC_CBO_0_PERFEVTSEL0, 0, 0);
   set_msr_bitmap(MSR_UNC_CBO_0_PERFEVTSEL1, 0, 0);
   set_msr_bitmap(MSR_UNC_CBO_0_PERFEVTSEL2, 0, 0);
   set_msr_bitmap(MSR_UNC_CBO_0_PERFEVTSEL3, 0, 0);
   set_msr_bitmap(MSR_UNC_CBO_0_PERFEVTSEL4, 0, 0);
   set_msr_bitmap(MSR_UNC_CBO_0_PERFEVTSEL5, 0, 0);
   set_msr_bitmap(MSR_UNC_CBO_0_PERFEVTSEL6, 0, 0);
   set_msr_bitmap(MSR_UNC_CBO_0_PERFEVTSEL7, 0, 0);
}

static void
cgc_miscmsr_config(void) {
   /*
    * we need to keep hypervisor FS/GS in sync with guest, but guest
    * can read w/out interpretation.
    */
   set_msr_bitmap(MSR_FS_BASE, 0, 1);
   set_msr_bitmap(MSR_GS_BASE, 0, 1);

   /* just pass through reads: we don't care, nothing to hide */
   set_msr_bitmap(MSR_SWAPGS, 0, 0);
   set_msr_bitmap(MSR_EFER, 0, 1);
   set_msr_bitmap(MSR_STAR, 0, 1);
   set_msr_bitmap(MSR_LSTAR, 0, 1);
   set_msr_bitmap(MSR_CSTAR, 0, 1);
   set_msr_bitmap(MSR_SF_MASK, 0, 1);

   /* used by cpufreq/est.c */
   set_msr_bitmap(MSR_IA32_PERF_STATUS, 0, 0);
   /* used by x86/tsc.c for calibration */
   set_msr_bitmap(MSR_MPERF, 0, 0);
   set_msr_bitmap(MSR_APERF, 0, 0);
   
   /* We don't care about SYSENTER in hypervisor */
   set_msr_bitmap(MSR_IA32_SYSENTER_ESP, 0, 0);
   set_msr_bitmap(MSR_IA32_SYSENTER_EIP, 0, 0);
   set_msr_bitmap(MSR_IA32_SYSENTER_CS, 0, 0);
}

static void
cgc_mcamsr_config(void) {
   uint64_t cap, i;
   
   if (!(cpu_feature & CPUID_MCE))
      return;
   if (!(cpu_feature & CPUID_MCA))
      return;

   cap = MsrRead(MSR_MCG_CAP);

   set_msr_bitmap(MSR_MCG_CAP, 0, 0);
   set_msr_bitmap(MSR_MCG_CTL, 0, 0);
   set_msr_bitmap(MSR_MCG_STATUS, 0, 0);
   set_msr_bitmap(MSR_P5_MC_ADDR, 0, 0);
   set_msr_bitmap(MSR_P5_MC_TYPE, 0, 0);
   set_msr_bitmap(MSR_MC0_CTL_MASK, 0, 0);

   for (i = 0; i < (cap & MCG_CAP_COUNT); i++) {
      set_msr_bitmap(MSR_MC_ADDR(i), 0, 0);
      set_msr_bitmap(MSR_MC_CTL(i), 0, 0);
      set_msr_bitmap(MSR_MC_CTL2(i), 0, 0);
      set_msr_bitmap(MSR_MC_MISC(i), 0, 0);
      set_msr_bitmap(MSR_MC_STATUS(i), 0, 0);
   }
}

static void setup_control_32(VmxStruct *vmx) {
   /* Configure conditions that we wish to catch via the following VMCS control fields
      which configure certain MSRs in the guest
      VMX_PIN_VM_EXEC_CONTROLS
      VMX_PROC_VM_EXEC_CONTROLS
      VMX_PROC_VM_EXEC_CONTROLS2

      These are the related MSRs

      MSR_IA32_VMX_PINBASED_CTLS
      MSR_IA32_VMX_PROCBASED_CTLS
      MSR_IA32_VMX_PROCBASED_CTLS2
      MSR_IA32_VMX_EXIT_CTLS
      MSR_IA32_VMX_ENTRY_CTLS
   */
   uint32_t exit_arch = 0xffffffff;
   uint32_t entry_arch = 0xffffffff;
   uint32_t bits = 0;

   //disable vm exit by Extern-interrupt, NMI and Virtual NMI
   cgc_vmwrite(VMX_PIN_VM_EXEC_CONTROLS,
          computeMsrValue(0, MSR_IA32_VMX_PINBASED_CTLS));

   bits = CPU_BASED_ACTIVATE_SECONDARY | CPU_BASED_RDTSC_EXITING |
      CPU_BASED_ACTIVATE_MSR_BITMAP | CPU_BASED_UNCOND_IO_EXITING;
   cgc_vmwrite(VMX_PROC_VM_EXEC_CONTROLS,
          computeMsrValue(bits, MSR_IA32_VMX_PROCBASED_CTLS));

//   cgc_vmwrite(VMX_EXCEPTION_BITMAP, 0);  //no exceptions cause exits for now
   cgc_vmwrite(VMX_EXCEPTION_BITMAP, 1 << 12);  //exit on trap 12

   cgc_vmwrite(VMX_PF_EC_MASK, 0);
   cgc_vmwrite(VMX_PF_EC_MATCH, 0);
   cgc_vmwrite(VMX_CR3_TARGET_COUNT, 0); //count must be <= 3

   // stop counters during vmexit
   bits = VMEXIT_CONTROL_ACK_INTR_ONEXIT |
      VMEXIT_CONTROL_LOAD_PERF_GLOBAL |
      VMEXIT_CONTROL_HOST_ADDR_SPACE;
   cgc_vmwrite(VMX_EXIT_CONTROLS,
          exit_arch & computeMsrValue(bits, MSR_IA32_VMX_EXIT_CTLS));
   printf("<cgcvmx> exit controls 0x%x (0x%x)\n", (uint32_t)vm_read(VMX_EXIT_CONTROLS), exit_arch);

   bits = VMENTRY_CONTROL_LOAD_PERF_GLOBAL |
      VMENTRY_CONTROL_IA32E_MODE_GUEST;
   cgc_vmwrite(VMX_ENTRY_CONTROLS,
          entry_arch & computeMsrValue(bits, MSR_IA32_VMX_ENTRY_CTLS));
   printf("<cgcvmx> entry controls 0x%x\n", (uint32_t)vm_read(VMX_ENTRY_CONTROLS));

   //use for event injection
   cgc_vmwrite(VMX_ENTRY_INT_INFO_FIELD, 0);

   //used if 0x80000800 are set in VMX_ENTRY_INT_INFO_FIELD
   cgc_vmwrite(VMX_ENTRY_EXCEPTION_EC, 0);

   //for software interrupts or exceptions
   cgc_vmwrite(VMX_ENTRY_INSTR_LENGTH, 0);

   cgc_vmwrite(VMX_TPR_THRESHOLD, 0);

   //intercept rdrand and descriptor table register access
   cgc_vmwrite(VMX_PROC_VM_EXEC_CONTROLS2,
          computeMsrValue(CPU2_BASED_RDRAND_EXITING |
                CPU2_BASED_RDSEED_EXITING |
                CPU2_BASED_ENABLE_RDTSCP |
                CPU2_BASED_DESCR_TABLE_EXITING,
                MSR_IA32_VMX_PROCBASED_CTLS2));

   if (one_setting_allowed(MSR_IA32_VMX_PROCBASED_CTLS2,
			   CPU2_BASED_PAUSE_LOOP_EXITING)) {
      cgc_vmwrite(VMX_PLE_GAP, 0);
      cgc_vmwrite(VMX_PLE_WINDOW, 0);
   }
}

static void setup_control_64(VmxStruct *vmx) {
   if (one_setting_allowed(MSR_IA32_VMX_PROCBASED_CTLS,
			   CPU_BASED_ACTIVATE_IO_BITMAP)) {
      //have not enabled I/O bitmaps in VMX_PROC_VM_EXEC_CONTROLS
      cgc_vmwrite(VMX_IO_BITMAP_A_FULL, 0);
      //have not enabled I/O bitmaps in VMX_PROC_VM_EXEC_CONTROLS
      cgc_vmwrite(VMX_IO_BITMAP_B_FULL, 0);
   }
   
   if (msr_bitmap_region)
      cgc_vmwrite(VMX_MSR_BITMAP_FULL, vtophys(msr_bitmap_region));

   if (vmx->vmexit_load_msr.mls_p)
      cgc_vmwrite(VMX_EXIT_MSR_LOAD_ADDR_FULL,
		  vtophys(vmx->vmexit_load_msr.mls_p));
   cgc_vmwrite(VMX_EXIT_MSR_LOAD_COUNT, vmx->vmexit_load_msr.mls_n);

   if (vmx->vmexit_store_msr.mls_p)
      cgc_vmwrite(VMX_EXIT_MSR_STORE_ADDR_FULL, 
		  vtophys(vmx->vmexit_store_msr.mls_p));
   cgc_vmwrite(VMX_EXIT_MSR_STORE_COUNT, vmx->vmexit_store_msr.mls_n);

   if (vmx->vmentry_load_msr.mls_p)
      cgc_vmwrite(VMX_ENTRY_MSR_LOAD_ADDR_FULL,
		  vtophys(vmx->vmentry_load_msr.mls_p));
   cgc_vmwrite(VMX_ENTRY_MSR_LOAD_COUNT, vmx->vmentry_load_msr.mls_n);

   cgc_vmwrite(VMX_EXECUTIVE_VMCS_PTR_FULL, 0);
   cgc_vmwrite(VMX_TSC_OFFSET_FULL, 0);

   if (one_setting_allowed(MSR_IA32_VMX_PROCBASED_CTLS2,
			   CPU2_BASED_VIRTUALIZE_APIC))
      cgc_vmwrite(VMX_VIRTUAL_APIC_PAGE_ADDR_FULL, 0);

   if (one_setting_allowed(MSR_IA32_VMX_PROCBASED_CTLS2,
			   CPU2_BASED_APIC_REG_VIRT))
      cgc_vmwrite(VMX_APIC_ACCESS_ADDR_FULL, 0);

   if (one_setting_allowed(MSR_IA32_VMX_PINBASED_CTLS,
			   PIN_BASED_POSTED_INTRS))
      cgc_vmwrite(VMX_POSTED_INT_DESCR_FULL, 0);

   if (one_setting_allowed(MSR_IA32_VMX_PROCBASED_CTLS2,
			   CPU2_BASED_ENABLE_VMFUNC))
      cgc_vmwrite(VMX_VMFUNC_CONTROLS_FULL, 0);

   if (one_setting_allowed(MSR_IA32_VMX_PROCBASED_CTLS2,
			   CPU2_BASED_ENABLE_EPT))
      cgc_vmwrite(VMX_EPT_POINTER_FULL, 0);

   if (one_setting_allowed(MSR_IA32_VMX_PROCBASED_CTLS2,
			   CPU2_BASED_VIRTUAL_INTR_DELIV)) {
      cgc_vmwrite(VMX_EOI_EXIT0_BITMAP_FULL, 0);
      cgc_vmwrite(VMX_EOI_EXIT1_BITMAP_FULL, 0);
      cgc_vmwrite(VMX_EOI_EXIT2_BITMAP_FULL, 0);
      cgc_vmwrite(VMX_EOI_EXIT3_BITMAP_FULL, 0);
   }

   if (one_setting_allowed(MSR_IA32_VMX_PROCBASED_CTLS,
			   CPU_BASED_ACTIVATE_SECONDARY) &&
       one_setting_allowed(MSR_IA32_VMX_PROCBASED_CTLS2,
			   CPU2_BASED_ENABLE_VMFUNC) &&
       (MsrRead(MSR_IA32_VMX_VMFUNC) & VMX_VMFUNC_EPTP_SWITCH)) {
      cgc_vmwrite(VMX_EPTP_LIST_FULL, 0);
   }
   
   if (one_setting_allowed(MSR_IA32_VMX_PROCBASED_CTLS2,
			   CPU2_BASED_VMCS_SHADOW)) {
      cgc_vmwrite(VMX_VMREAD_BITMAP_FULL, 0);
      cgc_vmwrite(VMX_VMWRITE_BITMAP_FULL, 0);
   }
   
   if (one_setting_allowed(MSR_IA32_VMX_PROCBASED_CTLS2,
			   CPU2_BASED_EPT_VIOLATION))
      cgc_vmwrite(VMX_VIRT_EXC_INFO_ADDR_FULL, 0);

   if (one_setting_allowed(MSR_IA32_VMX_PROCBASED_CTLS2,
			   CPU2_BASED_ENABLE_XSAVES))
      cgc_vmwrite(VMX_XSS_EXITING_BITMAP_FULL, 0);
}

static void setup_control_natural(void) {
   int i, n;
   
   /*
    * mask off all of the reserved bits in CR0/CR4, attempts to change them
    * from zero will cause VMexit. Reads will return 0 (from shadow registers).
    * This affects CLTS, LMSW, MOV CR[04], SMSW.
    */
   cgc_vmwrite(VMX_CR0_MASK, 0xffffffff1ffaffc0);
   cgc_vmwrite(VMX_CR0_READ_SHADOW, 0);
   cgc_vmwrite(VMX_CR4_MASK, 0xffffffffffd89800);
   cgc_vmwrite(VMX_CR4_READ_SHADOW, 0);
   
   n = VMX_MISC_CR3_TARGETS(MsrRead(MSR_IA32_VMX_MISC));
   for (i = 0; i < n; i++)
      cgc_vmwrite(VMX_CR3_TARGET_0 + (i * 2), 0);
}

static void setup_guest_16(void) {
   cgc_vmwrite(VMX_GUEST_ES_SEL, RegGetEs());
   cgc_vmwrite(VMX_GUEST_CS_SEL, RegGetCs());
   cgc_vmwrite(VMX_GUEST_SS_SEL, RegGetSs());
   cgc_vmwrite(VMX_GUEST_DS_SEL, RegGetDs());
   cgc_vmwrite(VMX_GUEST_FS_SEL, RegGetFs());
   cgc_vmwrite(VMX_GUEST_GS_SEL, RegGetGs());
   cgc_vmwrite(VMX_GUEST_TR_SEL, GetTrSelector());
   cgc_vmwrite(VMX_GUEST_LDTR_SEL, GetLdtr());

   if (one_setting_allowed(MSR_IA32_VMX_PROCBASED_CTLS2,
			   CPU2_BASED_VIRTUAL_INTR_DELIV))
      cgc_vmwrite(VMX_GUEST_INT_STATUS, 0);
}

static void setup_guest_32(void) {
   uint32_t access;
   uint32_t limit;
   Segment seg;
   uint64_t gdt_base;

   gdt_base = GetGdtBase();

   segFromSelector(&seg, RegGetEs(), (uint8_t*)gdt_base);
   cgc_vmwrite(VMX_GUEST_ES_LIMIT, seg.limit);

   segFromSelector(&seg, RegGetCs(), (uint8_t*)gdt_base);
   cgc_vmwrite(VMX_GUEST_CS_LIMIT, seg.limit);

   segFromSelector(&seg, RegGetSs(), (uint8_t*)gdt_base);
   cgc_vmwrite(VMX_GUEST_SS_LIMIT, seg.limit);

   segFromSelector(&seg, RegGetDs(), (uint8_t*)gdt_base);
   cgc_vmwrite(VMX_GUEST_DS_LIMIT, seg.limit);

   segFromSelector(&seg, RegGetFs(), (uint8_t*)gdt_base);
   cgc_vmwrite(VMX_GUEST_FS_LIMIT, seg.limit);

   segFromSelector(&seg, RegGetGs(), (uint8_t*)gdt_base);
   cgc_vmwrite(VMX_GUEST_GS_LIMIT, seg.limit);

   if (GetLdtr() != 0) {
      segFromSelector(&seg, GetLdtr(), (uint8_t*)gdt_base);
      cgc_vmwrite(VMX_GUEST_LDTR_LIMIT, seg.limit);
      cgc_vmwrite(VMX_GUEST_LDTR_BASE, seg.base);
      cgc_vmwrite(VMX_GUEST_LDTR_ATTR, seg.attr);
      cgc_vmwrite(VMX_GUEST_LDTR_SEL, GetLdtr());
   }
   else {
      cgc_vmwrite(VMX_GUEST_LDTR_LIMIT, 0);
      cgc_vmwrite(VMX_GUEST_LDTR_ATTR, 0x10000);
   }

   limit = GetTrLimit();
   cgc_vmwrite(VMX_GUEST_TR_LIMIT, limit);

   limit = GetGdtLimit();
   cgc_vmwrite(VMX_GUEST_GDTR_LIMIT, limit);

   limit = GetIdtLimit();
   cgc_vmwrite(VMX_GUEST_IDTR_LIMIT, limit);

   access = makeValidAccessRights(RegGetEs());
   cgc_vmwrite(VMX_GUEST_ES_ATTR, access);

   access = makeValidAccessRights(RegGetCs());
   cgc_vmwrite(VMX_GUEST_CS_ATTR, access);

   access = makeValidAccessRights(RegGetSs());
   cgc_vmwrite(VMX_GUEST_SS_ATTR, access);

   access = makeValidAccessRights(RegGetDs());
   cgc_vmwrite(VMX_GUEST_DS_ATTR, access);

   access = makeValidAccessRights(RegGetFs());
   cgc_vmwrite(VMX_GUEST_FS_ATTR, access);

   access = makeValidAccessRights(RegGetGs());
   cgc_vmwrite(VMX_GUEST_GS_ATTR, access);

   access = (GetTrAttr() >> 8) & 0xf0ff;
   cgc_vmwrite(VMX_GUEST_TR_ATTR, access);

   cgc_vmwrite(VMX_GUEST_INTERRUPTIBILITY_INFO, 0);
   cgc_vmwrite(VMX_GUEST_ACTIVITY_STATE, 0);
   cgc_vmwrite(VMX_GUEST_SMBASE, 0);

   cgc_vmwrite(VMX_GUEST_IA32_SYSENTER_CS, MsrRead(MSR_IA32_SYSENTER_CS));

   if (one_setting_allowed(MSR_IA32_VMX_PINBASED_CTLS,
			   PIN_BASED_ACTIVATE_PREEMPT_TIMER))
      cgc_vmwrite(VMX_GUEST_TIMER, 0);
}

static void setup_guest_64(void) {
   cgc_vmwrite(VMX_VMS_LINK_PTR_FULL, 0xffffffffffffffffull);
   cgc_vmwrite(VMX_GUEST_IA32_DEBUGCTL_FULL,
	       MsrRead(MSR_IA32_DEBUGCTL));

   if (one_setting_allowed(MSR_IA32_VMX_ENTRY_CTLS,
			   VMENTRY_CONTROL_LOAD_PERF_GLOBAL)) {
      cgc_vmwrite(VMX_GUEST_IA32_PERF_CTL_FULL,
		  MsrRead(MSR_IA32_PERF_GLOBAL_CTRL));
   }
}

static void setup_guest_natural(uint64_t guestRsp) {
   uint64_t gdt_base;
   uint64_t idt_base;
   Segment seg;

   cgc_vmwrite(VMX_GUEST_CR0, RegGetCr0());
   cgc_vmwrite(VMX_GUEST_CR3, RegGetCr3());
   cgc_vmwrite(VMX_GUEST_CR4, RegGetCr4());
   
   gdt_base = GetGdtBase();

   segFromSelector(&seg, RegGetCs(), (uint8_t*)gdt_base);
   cgc_vmwrite(VMX_GUEST_CS_BASE, seg.base);

   segFromSelector(&seg, RegGetEs(), (uint8_t*)gdt_base);
   cgc_vmwrite(VMX_GUEST_ES_BASE, seg.base);

   segFromSelector(&seg, RegGetSs(), (uint8_t*)gdt_base);
   cgc_vmwrite(VMX_GUEST_SS_BASE, seg.base);

   segFromSelector(&seg, RegGetDs(), (uint8_t*)gdt_base);
   cgc_vmwrite(VMX_GUEST_DS_BASE, seg.base);

   cgc_vmwrite(VMX_GUEST_FS_BASE, MsrRead(MSR_FS_BASE));

   cgc_vmwrite(VMX_GUEST_GS_BASE, MsrRead(MSR_GS_BASE));

   if (segFromSelector(&seg, GetTrSelector(), (uint8_t*)gdt_base) == 0) {
      cgc_vmwrite(VMX_GUEST_TR_BASE, seg.base);
   }
   else {
      printf("<cgcvmx> failed to compute TR base\n");
   }

   if (GetLdtr() != 0) {
      if (segFromSelector(&seg, GetLdtr(), (uint8_t*)gdt_base)) {
         cgc_vmwrite(VMX_GUEST_LDTR_BASE, 0);
      }
      else {
         cgc_vmwrite(VMX_GUEST_LDTR_BASE, seg.base);
      }
   }
   else {
      cgc_vmwrite(VMX_GUEST_LDTR_BASE, 0);
   }

   cgc_vmwrite(VMX_GUEST_GDTR_BASE, gdt_base);

   idt_base = GetIdtBase();

   cgc_vmwrite(VMX_GUEST_IDTR_BASE, idt_base);
   printf("<cgcvmx> IDTR base set to 0x%lx\n", idt_base);

   cgc_vmwrite(VMX_GUEST_DR7, 0x400);

   //host needs its own stack initialized with data it will need for vm exits
   //would like guest to appear to be returning from loading this .ko
   //*** need to consider proper values here
   printf("<cgcvmx> Setting VMX_GUEST_RSP\n");
   cgc_vmwrite(VMX_GUEST_RSP, guestRsp);
   {
      uint64_t rflags = RegGetRflags();
      cgc_vmwrite(VMX_GUEST_RFLAGS, rflags);
      printf("<cgcvmx> Setting VMX_GUEST_RFLAGS %lx\n", rflags);
   }
//   cgc_vmwrite(VMX_GUEST_RFLAGS, 2);
   cgc_vmwrite(VMX_GUEST_PENDING_DEBUG_EXCEPT, 0);
   cgc_vmwrite(VMX_GUEST_IA32_SYSENTER_ESP, MsrRead(MSR_IA32_SYSENTER_ESP));
   cgc_vmwrite(VMX_GUEST_IA32_SYSENTER_EIP, MsrRead(MSR_IA32_SYSENTER_EIP));
}

static void setup_host_16(void) {
/*
   from Vol3, 26.2.3: Checks on Host Segment and Descriptor-Table Registers
   In the selector field for each of CS, SS, DS, ES, FS, GS and TR,
   the RPL (bits 1:0) and the TI flag (bit 2) must be 0.
   should really & ~3 on each selector
*/
   cgc_vmwrite(VMX_HOST_ES_SEL, RegGetEs() & ~3);
   cgc_vmwrite(VMX_HOST_CS_SEL, RegGetCs() & ~3);
   cgc_vmwrite(VMX_HOST_SS_SEL, RegGetSs() & ~3);
   cgc_vmwrite(VMX_HOST_DS_SEL, RegGetDs() & ~3);
   cgc_vmwrite(VMX_HOST_FS_SEL, RegGetFs() & ~3);
   cgc_vmwrite(VMX_HOST_GS_SEL, RegGetGs() & ~3);
   cgc_vmwrite(VMX_HOST_TR_SEL, GetTrSelector() & ~3);
}

static void setup_host_32(void) {
   cgc_vmwrite(VMX_HOST_IA32_SYSENTER_CS,
	       MsrRead(MSR_IA32_SYSENTER_CS) & 0xffff);
}

static void setup_host_64(void) {
   if (one_setting_allowed(MSR_IA32_VMX_EXIT_CTLS,
			   VMEXIT_CONTROL_LOAD_PERF_GLOBAL)) {
      cgc_vmwrite(VMX_HOST_IA32_PERF_CTL_FULL, 0);
   }
   
   if (one_setting_allowed(MSR_IA32_VMX_EXIT_CTLS,
			   VMEXIT_CONTROL_LOAD_IA32_PAT)) {
      cgc_vmwrite(VMX_HOST_IA32_PAT_FULL, MsrRead(MSR_IA32_PAT));
   }

   if (one_setting_allowed(MSR_IA32_VMX_EXIT_CTLS,
			   VMEXIT_CONTROL_LOAD_IA32_EFER)) {
      cgc_vmwrite(VMX_HOST_IA32_EFER_FULL, MsrRead(MSR_IA32_EFER));
   }
}

static void setup_host_natural(VmxStruct *vmx) {
   uint64_t gdt_base;
   uint64_t idt_base;
   Segment seg;

   cgc_vmwrite(VMX_HOST_CR0, RegGetCr0());

   // host always uses kernel page tables (non-PAE) or page directory
   // table (PAE).  Whichever it is, it should be the thing in CR3
   // right now since we're in the kernel.
   printf("<cgcvmx> HOST CR3 %p\n", (void*)RegGetCr3());
   cgc_vmwrite(VMX_HOST_CR3, RegGetCr3());

   printf("<cgcvmx> HOST CR4 %p\n", (void*)RegGetCr4());
   cgc_vmwrite(VMX_HOST_CR4, RegGetCr4());

   cgc_vmwrite(VMX_HOST_FS_BASE, MsrRead(MSR_FS_BASE));
   cgc_vmwrite(VMX_HOST_GS_BASE, MsrRead(MSR_GS_BASE));

   gdt_base = GetGdtBase();
   cgc_vmwrite(VMX_HOST_GDTR_BASE, gdt_base);

   if (segFromSelector(&seg, GetTrSelector(), (uint8_t*)gdt_base) == 0) {
      cgc_vmwrite(VMX_HOST_TR_BASE, seg.base);
   }
   else {
      printf("<cgcvmx> failed to compute TR base\n");
   }

   idt_base = GetIdtBase();
   printf("<cgcvmx> idtbase is at %p\n", (void*)idt_base);

   cgc_vmwrite(VMX_HOST_IDTR_BASE, idt_base);

   cgc_vmwrite(VMX_HOST_IA32_SYSENTER_ESP, MsrRead(MSR_IA32_SYSENTER_ESP));
   cgc_vmwrite(VMX_HOST_IA32_SYSENTER_EIP, MsrRead(MSR_IA32_SYSENTER_EIP));

   //host needs its own stack initialized with data it will need for vm exits
   //would like guest to appear to be returning from loading this lkm
   //*** need to consider proper values here
   cgc_vmwrite(VMX_HOST_RSP, (uintptr_t)&vmx->host_stack);

}

static void initialize_vmcs(VmxStruct *vmx, uint64_t guestRsp) {
   setup_control_16();
   setup_control_32(vmx);
   setup_control_64(vmx);
   setup_control_natural();

   setup_guest_16();
   setup_guest_32();
   setup_guest_64();
   setup_guest_natural(guestRsp);

   setup_host_16();
   setup_host_32();
   setup_host_64();
   setup_host_natural(vmx);
}

/* Allocate vmcs region for the guest */
static int allocate_vmcs_region(VmxStruct *vmx) {
   if (vmx == NULL) {
      return -1;
   }
   if ((vmx->vmcs_guest_region = malloc(PAGE_SIZE, M_VMCS_GUEST, M_ZERO|M_WAITOK)) == NULL) {
      return -1;
   }
   printf("<cgcvmx> allocated vmcs guest region for cpu %u at %p\n", vmx->procNum, vmx->vmcs_guest_region);
/*
   if ((vmx->io_bitmap_a_region = malloc(PAGE_SIZE, M_VMCS_GUEST, M_ZERO|M_WAITOK)) == NULL) {
      return -1;
   }
   if ((vmx->io_bitmap_b_region = malloc(PAGE_SIZE, M_VMCS_GUEST, M_ZERO|M_WAITOK)) == NULL) {
      return -1;
   }
*/
   //
   // if ((vmx->virtual_apic_page = malloc(PAGE_SIZE, M_VMCS_MSR, M_WAITOK|M_ZERO)) == NULL)
   //   return -1;
   //
   return 0;
}

/* Dealloc vmcs guest region*/
static void deallocate_vmcs_region(VmxStruct *vmx) {
   if (vmx == NULL) {
      return;
   }
   if (vmx->vmcs_guest_region) {
      printf("<cgcvmx> freeing allocated vmcs region!\n");
      free(vmx->vmcs_guest_region, M_VMCS_GUEST);
   }
/*
   if (vmx->io_bitmap_a_region) {
      printf("<cgcvmx> freeing allocated io bitmapA region!\n");
      free(vmx->io_bitmap_a_region);
   }
   if (vmx->io_bitmap_b_region) {
      printf("<cgcvmx> freeing allocated io bitmapB region!\n");
      free(vmx->io_bitmap_b_region);
   }
*/

   // XXX reference count and free on last vm deallocate
   free(msr_bitmap_region, M_VMCS_MSR);
   msr_bitmap_region = NULL;

   free(vmx->vmexit_store_msr.mls_p, M_VMCS_MSR);
   vmx->vmexit_store_msr.mls_p = NULL;
   vmx->vmexit_store_msr.mls_n = 0;

   free(vmx->vmexit_load_msr.mls_p, M_VMCS_MSR);
   vmx->vmexit_load_msr.mls_p = NULL;
   vmx->vmexit_load_msr.mls_n = 0;

   free(vmx->vmentry_load_msr.mls_p, M_VMCS_MSR);
   vmx->vmentry_load_msr.mls_p = NULL;
   vmx->vmentry_load_msr.mls_n = 0;

/*
   if (vmx->virtual_apic_page) {
      printf("<cgcvmx> freeing allocated virtual apic page region!\n");
      free(vmx->virtual_apic_page);
   }
*/
   free(vmx->vmxon_region, M_VMXON);
   free(vmx->host_stack, M_HOSTSTACK);
}

/*turn on vmxe*/
static void turn_on_vmxe(uint32_t procNum) {
   cr4_set(X86_CR4_VMXE);
   printf("<cgcvmx> turned on cr4.vmxe for CPU %u\n", procNum);
}

/*turn off vmxe*/
static void turn_off_vmxe(void) {
   cr4_reset(X86_CR4_VMXE);
   printf("<cgcvmx> turned off cr4.vmxe\n");
}

/*do vmptrld*/
static void cgc_vmptrld(VmxStruct *vmx) {
   if (vm_ptrld(vtophys(vmx->vmcs_guest_region)) != 0) {
      printf("<cgcvmx> vmptrld has failed!\n");
   }
   else {
      printf("<cgcvmx> vmptrld done!\n");
   }
}

/*do vmclear*/
static void cgc_vmclear(VmxStruct *vmx) {
   if (vm_clear(vtophys(vmx->vmcs_guest_region))) {
      printf("<cgcvmx> vmclear failed!\n");
   }
   else {
      printf("<cgcvmx> vmclear done!\n");
   }
}

/*do vmxon*/
static void cgc_vmxon(VmxStruct *vmx) {
   printf("<cgcvmx> attempting vmxon\n");
   vmx->failed = vmx_on(vtophys(vmx->vmxon_region));
   if (vmx->failed) {
      printf("<cgcvmx> vmxon has failed!\n");
   }
   else {
      printf("<cgcvmx> vmxon is on!\n");
   }
}

/*do vmxoff*/
static void cgc_vmxoff(uint32_t procNum) {
   uint32_t vmxoff_fail = vmx_off();
   if (vmxoff_fail) {
      printf("<cgcvmx> vmxoff has failed for CPU %u!\n", procNum);
   }
   printf("<cgcvmx> vmxoff complete for CPU %u\n", procNum);
}

static void procVmxOff(void *arg) {
   VmxStruct *vmx = (VmxStruct*)arg;
   if (vmx == NULL) { //not called from a thread context
      turn_off_vmxe();
      return;
   }
   //need clean way to do this acros all processors
   //can loop and vmxoff each one?
   //no global state shared across all CPUs
   //need to deallocate vmx across all CPUs
   //do we need to acquire/release sched_lock mutex around call to sched_bind ???
   sched_bind(curthread, vmx->procNum); //bind thread to cpu before virtualizing
   if (vmx->failed == 0) {
      printf("<cgcvmx> proc %u in vmxon: Attempting vmxoff\n", vmx->procNum);
      cgc_vmxoff(vmx->procNum);
   }
   turn_off_vmxe();
   deallocate_vmcs_region(vmx);
   kthread_exit();
}

static void cgcvmx_off(void) {
   //need clean way to do this acros all processors
   //can loop and vmxoff each one?
   //no global state shared across all CPUs
   //need to deallocate vmx across all CPUs
   int procNum;
   CPU_FOREACH (procNum) {
      int tres;
      struct thread *kthread;
      tres = kthread_add(procVmxOff, (void*)threadVmx[procNum], NULL, &kthread, RFSTOPPED, 0, "vmxoff.%u", procNum);
      if (tres != 0) {
         printf("<cgcvmx> Failed to bind thread to CPU %d\n", procNum);
         //need a graceful way to abort here
      }
   }
}

int real_cgcvmx_on(uint64_t guestRip, uint64_t guestRsp, uint32_t procNum) {
   uint32_t msr3a_value = 0;
   uint32_t vmx_rev_id = 0;
   uint32_t res;
   CpuIdRegs regs;
   uint8_t *hostStack;
   VmxStruct *vmx;

   printf("<cgcvmx> In cgcvmx_on for proc %u\n", procNum);

   regs.eax = 1;
   regs.ecx = 0;
   cgc_cpuid(&regs);

   if ((regs.ecx >> CPUID_VMX_BIT) & 1) {
      printf("<cgcvmx> VMX supported CPU.\n");
   }
   else {
      printf("<cgcvmx> VMX not supported by CPU. Not doing anything\n");
      goto success;
   }

   msr3a_value = (uint32_t)MsrRead(MSR_IA32_FEATURE_CONTROL);
   switch (msr3a_value & 5) {
      case 0:
         printf("<cgcvmx> MSR 0x3A: VMXON bit is off, Lock bit is not on. Enabling and locking MSR\n");
         MsrWrite(MSR_IA32_FEATURE_CONTROL, msr3a_value | 4);
         msr3a_value = (uint32_t)MsrRead(MSR_IA32_FEATURE_CONTROL);
         MsrWrite(MSR_IA32_FEATURE_CONTROL, msr3a_value | 1);
         break;
      case 1:
         printf("<cgcvmx> MSR 0x3A: Lock bit is on. VMXON bit is off. Cannot do vmxon\n");
         goto success;
      case 4:
         printf("<cgcvmx> MSR 0x3A: VMXON bit is on, Lock bit is not on. Locking MSR\n");
         MsrWrite(MSR_IA32_FEATURE_CONTROL, msr3a_value | 1);
         break;
      case 5:
         printf("<cgcvmx> MSR 0x3A: Lock bit is on. VMXON bit is on. OK\n");
         break;
   }

   res = (uint32_t)MsrRead(MSR_IA32_EFER);
   printf("<cgcvmx> MSR_IA32_EFER - 0x%x\n", res);

   hostStack = malloc(STACK_PAGES * PAGE_SIZE, M_HOSTSTACK, M_WAITOK|M_ZERO);
   if (hostStack == NULL) {
      printf("<cgcvmx> Error allocating host stack\n");
      cgcvmx_off();
      return -ENOMEM;
   }

   printf("<cgcvmx> host stack for cpu %u allocated at %p\n", procNum, hostStack);

   vmx = (VmxStruct*)(hostStack + STACK_PAGES * PAGE_SIZE - sizeof(VmxStruct));
   memset(vmx, 0, sizeof(VmxStruct));

   threadVmx[procNum] = vmx;
   vmx->procNum = procNum;
   vmx->host_stack = hostStack;

   vmx->vmxon_region = malloc(PAGE_SIZE, M_VMXON, M_ZERO | M_WAITOK);

   if (vmx->vmxon_region == NULL) {
      goto memfail;
   }
   printf("<cgcvmx> allocated vmxon region for cpu %u at %p\n", procNum, vmx->vmxon_region);

   vmx_rev_id = (uint32_t)MsrRead(MSR_IA32_VMX_BASIC);

   memcpy(vmx->vmxon_region, &vmx_rev_id, 4); //copy revision id to vmxon region

   turn_on_vmxe(procNum);

   cgc_vmxon(vmx);

   if (allocate_vmcs_region(vmx) != 0) {
      goto memfail;
   }

   //need to do this first
   cgc_vmclear(vmx);
   //then set revision identifier
   memcpy(vmx->vmcs_guest_region, &vmx_rev_id, 4); //copy revision id to vmcs region
   //then make the reagion active
   cgc_vmptrld(vmx);

   msrloadstore_init(vmx);

   initialize_vmcs(vmx, guestRsp);

   printf("<cgcvmx> vmexit_handler at %p\n", vmexit_handler);

   //host rip
   cgc_vmwrite(VMX_HOST_RIP, (uint64_t)vmexit_handler);

   //guest rip
   cgc_vmwrite(VMX_GUEST_RIP, guestRip);  //guest resume point
   //guest needs to appear to be returning from loading this module ???

   printf("<cgcvmx> VMX_GUEST_RSP set to %p\n", (void*)vm_read(VMX_GUEST_RSP));
   printf("<cgcvmx> VMX_GUEST_RIP set to %p\n", (void*)vm_read(VMX_GUEST_RIP));
   printf("<cgcvmx> hang on, about to launch\n");
   res = vm_launch();
   if (res != 0) {
      uint32_t inst_error = vm_read(VMX_INSTR_ERROR);
      printf("<cgcvmx> vmlaunch failed: %x, error: 0x%x\n", res, inst_error);
   }
   else {
      //host never gets here, host will next run at vmexit_handler on a vm_exit
      //guest resumes at VMX_GUEST_RIP with VMX_GUEST_RSP
      //want that to appear to be returning from initializing this module ???
      printf("<cgcvmx> If you see this, vmlaunch failed\n");
   }

   printf("<cgcvmx> Finished vmxon\n");

success:
   return 0;

memfail:
   printf("<cgcvmx> Error allocating cgcvmx memory\n");
   procVmxOff(vmx);
   threadVmx[procNum] = NULL;
   return -ENOMEM;
}

//arg is cpu
static void virtualizeProc(void *arg) {
   //do we need to acquire/release sched_lock mutex around call to sched_bind ???
   struct thread *td;
   
   int cpu = (int) ((intptr_t) arg) & 0xFFFFFFFF;

   td = curthread;
   thread_lock(td);
   sched_bind(td, cpu); //bind thread to cpu before virtualizing
   thread_unlock(td);
   if (init_tramp(cpu))
      printf("<cgcvmx> vmlaunch failed\n");
   else
      printf("<cgcvmx> made it back to cgcvmx_on, we are in the matrix if we don't crash\n");
   kthread_exit();
}

static int cgcvmx_on(void) {
   uint64_t procNum;

   /* all MSR access defaults to generating vmexit */
   if (one_setting_allowed(MSR_IA32_VMX_PROCBASED_CTLS,
			   CPU_BASED_ACTIVATE_MSR_BITMAP)) {
      msr_bitmap_region = malloc(PAGE_SIZE, M_VMCS_MSR, M_WAITOK);
      memset(msr_bitmap_region, -1, PAGE_SIZE);
   }
   
   cgc_pmcmsr_config();
   cgc_mcamsr_config();
   cgc_miscmsr_config();

   threadVmx = (VmxStruct**)malloc(sizeof(VmxStruct*) * MAXCPU, M_VMXARRAY, M_ZERO | M_WAITOK);
   //use kthreads to setup vmcs for each core and vmlaunch
   //otherwise vmlaunch will never return when used on first core
   //also need to pin threads to specific cores to make sure we vmxon and vmlaunch
   //across all available cores
   CPU_FOREACH (procNum) {
      int tres;
      struct thread *kthread;
      printf("<cgcvmx> virtualizing processor #%p\n", (void*)procNum);
      tres = kthread_add(virtualizeProc, (void*)procNum, NULL, &kthread, 0, 0, "vmx.%u", (uint32_t)procNum);
      if (tres != 0) {
         printf("<cgcvmx> Failed to bind thread to CPU %p\n", (void*)procNum);
         //need a graceful way to abort here
      } 
   }
   return 0;
}

static int evtHandler(struct module *mod, int evt, void *arg) {
   // Set return code to 0
   int res = 0;
   switch (evt) {
      case MOD_LOAD:
         res = cgcvmx_on();
         break;
      case MOD_UNLOAD:
         cgcvmx_off();
         break;
      default:
         res = EOPNOTSUPP;
         break;
   }
   return (res);
}

static void
set_msr_bitmap(uint32_t msr, int r, int w) {
   uint32_t index;
   
   if (msr_bitmap_region == NULL)
      return;

   if (msr < 0x2000)
      index = (msr & 0x1fff) + 0x0000;
   else if (msr >= 0xc0000000 && msr < 0xc0002000)
      index = (msr & 0x1fff) + 0x2000;
   else {
      printf("<cgcvmx> set_msr_bitmap: invalid msr 0x%x\n", msr);
      return;
   }

   if (r)
      msr_bitmap_region[index >> 3] |= 1 << (index & 7);
   else
      msr_bitmap_region[index >> 3] &= ~(1 << (index & 7));
   
   index += 0x4000;

   if (w)
      msr_bitmap_region[index >> 3] |= 1 << (index & 7);
   else
      msr_bitmap_region[index >> 3] &= ~(1 << (index & 7));
}

static void
vmexit_wrmsr(VmxStruct *vmx) {
   uint64_t val;
   int ignored = 0;
   
   val = (vmx->rdx << 32) | (vmx->rax & 0xffffffff);
   switch (vmx->rcx) {
   case MSR_FS_BASE:
      cgc_vmwrite(VMX_GUEST_FS_BASE, val);
      cgc_vmwrite(VMX_HOST_FS_BASE, val);
      MsrWrite(vmx->rcx, val);
      break;
   case MSR_GS_BASE:
      cgc_vmwrite(VMX_GUEST_GS_BASE, val);
      cgc_vmwrite(VMX_HOST_GS_BASE, val);
      MsrWrite(vmx->rcx, val);
      break;
   case MSR_SWAPGS:
      MsrWrite(vmx->rcx, val);
      break;
   case MSR_IA32_PERF_GLOBAL_CTRL:
      cgc_vmwrite(VMX_GUEST_IA32_PERF_CTL_FULL, val);
      break;
   default:
      ignored = 1;
      break;
   }

   if (ignored)
      printf("<cgcvmx> wrmsr %lx <- %lx [ignored]\n", vmx->rcx, val);
}

static void
vmexit_rdmsr(VmxStruct *vmx) {
   uint64_t val;
   int ignored = 0;
   
   switch (vmx->rcx) {
   case MSR_FS_BASE:
      val = vm_read(VMX_GUEST_FS_BASE);
      break;
   case MSR_GS_BASE:
      val = vm_read(VMX_GUEST_GS_BASE);
      break;
   case MSR_SWAPGS:
   case MSR_EFER:
   case MSR_STAR:
   case MSR_LSTAR:
   case MSR_CSTAR:
   case MSR_SF_MASK:
      val = MsrRead(vmx->rcx);
      break;

   default:
      val = 0;
      ignored = 1;
      break;
   }

   vmx->rax = (val >> 0) & 0xffffffff;
   vmx->rdx = (val >> 32) & 0xffffffff;
   if (ignored)
      printf("<cgcvmx> rdmsr %lx [ignored]\n", vmx->rcx);
}

static void
msrloadstore_init(VmxStruct *vmx) {
   vmx->max_saved_msrs = VMX_MISC_MAX_SAVED_MSR(MsrRead(MSR_IA32_VMX_MISC));
   vmx->max_saved_msrs = (vmx->max_saved_msrs + 1) * 512;

   msrloadstore_init_entry(vmx, &vmx->vmexit_store_msr);
   msrloadstore_init_entry(vmx, &vmx->vmexit_load_msr);
   msrloadstore_init_entry(vmx, &vmx->vmentry_load_msr);
}

static void
msrloadstore_init_entry(VmxStruct *vmx, msrloadstore *mls) {
   mls->mls_n = 0;
   mls->mls_p = malloc(vmx->max_saved_msrs * sizeof(msrstore_entry),
		       M_VMCS_MSR, M_ZERO | M_WAITOK);
}

#if 0
static int
msrloadstore_add(VmxStruct *vmx, msrloadstore *mls, uint32_t msr) {
   return msrloadstore_addval(vmx, mls, msr, (uint64_t)0);
}

static int
msrloadstore_addval(VmxStruct *vmx, msrloadstore *mls, uint32_t msr, uint64_t val) {
   if (mls->mls_p == NULL)
      return (-1);
   if (mls->mls_n >= vmx->max_saved_msrs)
      return (-1);
   mls->mls_p[mls->mls_n].msr = msr;
   mls->mls_p[mls->mls_n].unused = 0;
   mls->mls_p[mls->mls_n].value = val;
   return mls->mls_n++;
}
#endif

static moduledata_t  cgcvmxData = {
   "cgcvmx",
   evtHandler,
   NULL
};

DECLARE_MODULE(cgcvmx, cgcvmxData, SI_SUB_SMP, SI_ORDER_ANY);

/* Local variables: */
/* mode: c */
/* c-basic-offset: 3 */
/* End: */
